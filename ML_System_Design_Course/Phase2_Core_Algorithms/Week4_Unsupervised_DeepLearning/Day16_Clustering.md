# Day 16: Clustering Algorithms

> **Phase**: 2 - Core Algorithms
> **Week**: 4 - Unsupervised & Deep Learning
> **Focus**: Finding Structure in Chaos
> **Reading Time**: 50 mins

---

## 1. Centroid-Based: K-Means

The most popular clustering algorithm. Simple, fast, but limited.

### 1.1 The Algorithm
1.  Initialize $K$ centroids randomly.
2.  Assign each point to the nearest centroid.
3.  Move centroids to the mean of their assigned points.
4.  Repeat until convergence.

### 1.2 Limitations
*   **Spherical Assumption**: Assumes clusters are spheres of equal variance. Fails on elongated or irregular shapes.
*   **K is unknown**: You must specify $K$.
*   **Outliers**: Sensitive. One outlier can pull the centroid significantly.

---

## 2. Density-Based: DBSCAN

Density-Based Spatial Clustering of Applications with Noise.

### 2.1 The Idea
Clusters are dense regions separated by sparse regions.
*   **Parameters**: $\epsilon$ (radius), min_samples.
*   **Core Point**: Has > min_samples within $\epsilon$.
*   **Noise**: Points in low-density regions are labeled as outliers (-1).

### 2.2 Pros/Cons
*   **Pros**: Can find arbitrary shapes (crescents, rings). Handles outliers automatically. No need to specify $K$.
*   **Cons**: Struggles with varying densities. $O(N^2)$ without spatial indexing.

---

## 3. Probabilistic: Gaussian Mixture Models (GMM)

"Soft K-Means".

### 3.1 The Model
Assumes data is generated by a mixture of $K$ Gaussian distributions.
*   **Soft Assignment**: Instead of "Point A is in Cluster 1", we say "Point A is 80% Cluster 1, 20% Cluster 2".
*   **Covariance**: Can model elliptical clusters (unlike K-Means spheres).
*   **Training**: Expectation-Maximization (EM) algorithm.

---

## 4. Real-World Challenges & Solutions

### Challenge 1: High Dimensionality
**Scenario**: Clustering documents (TF-IDF) with 10,000 features.
**Problem**: Distance metrics break down (Curse of Dimensionality). Everything is far from everything.
**Solution**:
*   Reduce dimensions first (PCA/UMAP).
*   Use Cosine Similarity instead of Euclidean Distance (Spherical K-Means).

### Challenge 2: Evaluation
**Scenario**: You run K-Means. You get clusters. Are they good?
**Problem**: No ground truth labels.
**Solution**:
*   **Silhouette Score**: Measures how close points are to their own cluster vs. neighbor clusters.
*   **Stability**: Run on random subsets. Do clusters stay the same?
*   **Business Validation**: Do the clusters make sense? (e.g., "High Spenders", "Window Shoppers").

---

## 5. Interview Preparation

### Conceptual Questions

**Q1: How do you select K in K-Means?**
> **Answer**:
> *   **Elbow Method**: Plot Inertia (Sum of Squared Distances) vs. K. Look for the "elbow" where improvement slows down.
> *   **Silhouette Analysis**: Pick K that maximizes the average Silhouette Score.
> *   **Domain Knowledge**: Business needs 3 customer segments (Low, Mid, High).

**Q2: Compare K-Means and GMM.**
> **Answer**: K-Means is a special case of GMM where:
> 1.  Covariance matrices are spherical and fixed.
> 2.  Assignments are hard (0 or 1).
> GMM is more flexible (ellipses, soft assignments) but slower and more prone to local minima.

**Q3: Why does DBSCAN struggle with varying densities?**
> **Answer**: DBSCAN uses a global $\epsilon$ (distance threshold). If Cluster A is tight (high density) and Cluster B is loose (low density), a small $\epsilon$ will find A but mark B as noise. A large $\epsilon$ will find B but merge A with other noise. OPTICS is an algorithm that solves this.

---

## 6. Further Reading
- [Visualizing DBSCAN](https://www.naftaliharris.com/blog/visualizing-dbscan-clustering/)
- [Gaussian Mixture Models Explained](https://jakevdp.github.io/PythonDataScienceHandbook/05.12-gaussian-mixtures.html)
