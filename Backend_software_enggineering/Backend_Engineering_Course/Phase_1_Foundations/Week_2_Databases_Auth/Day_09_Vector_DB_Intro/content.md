# Day 9: Intro to Vector Databases

## 1. The AI Database

Traditional databases (SQL/NoSQL) are great for **exact matches** (`WHERE id = 5`) or **keyword matches** (`LIKE '%apple%'`).
But how do you query for **meaning**?
*   *Query*: "Something delicious and red."
*   *SQL*: Fails.
*   *Vector DB*: Returns "Apple", "Strawberry", "Cherry".

### 1.1 What is an Embedding?
An embedding is a list of floating-point numbers (a vector) that represents the **semantic meaning** of a piece of text (or image/audio).
*   "Dog" -> `[0.1, 0.5, 0.2]`
*   "Puppy" -> `[0.1, 0.55, 0.21]` (Close to Dog)
*   "Car" -> `[0.9, 0.1, 0.0]` (Far from Dog)

These vectors are generated by **Embedding Models** (e.g., OpenAI `text-embedding-3-small`, HuggingFace `all-MiniLM-L6-v2`).

---

## 2. Similarity Search

Once we have vectors, we need to find the "nearest neighbors".

### 2.1 Distance Metrics
1.  **Cosine Similarity**: Measures the angle between two vectors.
    *   Range: -1 to 1. (1 = Identical direction).
    *   *Best for*: NLP (text similarity) where magnitude doesn't matter.
2.  **Euclidean Distance (L2)**: Measures the straight-line distance.
    *   *Best for*: Scenarios where magnitude matters.
3.  **Dot Product**: `A . B`.
    *   *Best for*: Recommender systems (Matrix Factorization).

### 2.2 The Indexing Problem
If you have 1 Million vectors, comparing the query vector to *all* 1M vectors (Brute Force / KNN) is too slow (`O(N)`).
We need an index.

### 2.3 HNSW (Hierarchical Navigable Small World)
This is the standard index algorithm for Vector DBs.
*   **Concept**: A multi-layered graph.
*   **Top Layer**: "Highways" (Long jumps across the dataset).
*   **Bottom Layer**: "Local Roads" (Fine-grained connections).
*   **Search**: Start at the top, jump close to the target, drop down a layer, refine, repeat.
*   **Performance**: `O(log N)`. Blazingly fast.

---

## 3. The Landscape

1.  **Pinecone**: Managed SaaS.
    *   *Pros*: Easy to use, scales well.
    *   *Cons*: Closed source, data leaves your premise.
2.  **Weaviate**: Open Source, Go-based.
    *   *Pros*: Built-in object storage, GraphQL API.
3.  **Qdrant**: Open Source, Rust-based.
    *   *Pros*: Extremely fast, great filtering support.
4.  **pgvector (Postgres Extension)**:
    *   *Pros*: It's just Postgres! ACID compliance, joins with relational data.
    *   *Cons*: Slower than specialized DBs at massive scale (100M+ vectors), though catching up.

---

## 4. RAG (Retrieval-Augmented Generation)

This is the #1 use case for Vector DBs in 2025.
1.  **Ingest**: Split your PDF/Docs into chunks.
2.  **Embed**: Send chunks to OpenAI -> Get Vectors.
3.  **Store**: Save Vectors + Text in Vector DB.
4.  **Query**: User asks a question. Embed the question.
5.  **Retrieve**: Find top 5 chunks similar to the question.
6.  **Generate**: Send chunks + question to LLM (GPT-4) to generate an answer.

---

## 5. Summary

Today we learned how machines understand meaning.
*   **Embeddings**: Text to Numbers.
*   **Vector DBs**: Specialized stores for similarity search.
*   **HNSW**: The algorithm that makes it fast.

**Tomorrow (Day 10)**: We wrap up Week 2 with **Auth & Security**. We'll learn how to secure these APIs and Databases we've been building using JWTs and Hashing.
