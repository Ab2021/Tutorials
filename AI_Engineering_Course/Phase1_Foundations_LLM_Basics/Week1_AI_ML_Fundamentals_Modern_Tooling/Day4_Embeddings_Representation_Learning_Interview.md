# Day 4: Embeddings & Representation Learning
##Interview Questions & Production Challenges

### Interview Questions

#### Q1: Explain the difference between static embeddings (Word2Vec) and contextual embeddings (BERT). When would you use each?

**Answer:**

**Static Embeddings (Word2Vec, GloVe, FastText):**

- **One vector per word type**, regardless of context
- Example: "bank" always has same embedding
  - "river bank" → [0.2, -0.1, 0.3, ...]
  - "savings bank" → [0.2, -0.1, 0.3, ...] ← Same!
- Trained using local context (Skip-Gram) or global co-occurrence (GloVe)
- Fast: O(1) lookup from embedding table

**Contextual Embeddings (ELMo, BERT, GPT):**

- **Different vector for same word in different contexts**
- Example: "bank" embeddings differ:
  - "river bank" → [0.5, 0.3, -0.2, ...]
  - "savings bank" → [-0.1, 0.6, 0.4, ...] ← Different!
- Generated by passing through neural network (LSTM, Transformer)
- Slower: O(sequence_length) forward pass required

**When to Use Static:**
- **Speed critical**: Real-time systems, edge devices
- **Small models**: Limited compute/memory
- **Simple tasks**: Sentiment analysis, topic classification
- **Interpretability**: Want to analyze word relationships
- **Example**: Spam classification (high throughput, simple features)

**When to Use Contextual:**
- **Ambiguity matters**: QA, NER, WSD (word sense disambiguation)
- **Complex semantics**: Entailment, reading comprehension
- **State-of-the-art performance**: When accuracy >> speed
- **Transfer learning**: Leverage pre-trained LLMs
- **Example**: Medical NER (distinguish "cold" the illness vs temperature)

**Hybrid Approach:**
- Use static for initial retrieval/filtering (fast)
- Use contextual for final ranking/classification (accurate)

---

#### Q2: You're training Word2Vec on medical text and find that rare medical terms have poor embeddings. How would you improve this?

**Answer:**

**Problem Diagnosis:**

Rare words have few training examples → noisy gradients → poor embeddings.

**Example:**
```
"hypertension" appears 1000 times → good embedding
"rhabdomyolysis" appears 10 times → poor embedding
```

**Solutions:**

**1. Use FastText Instead of Word2Vec:**

FastText uses character n-grams → rare words share subwords with common words.

```python
# "rhabdomyolysis" shares n-grams with:
# - "rhabdo" (common medical prefix)
# - "myolysis" (muscle breakdown)
# - Even if word is rare, n-grams appear in other medical terms

from gensim.models import FastText

model = FastText(
    sentences=medical_corpus,
    vector_size=300,
    window=5,
    min_count=1,  # Don't filter rare words!
    sg=1,  # Skip-gram
    word_ngrams=1  # Use n-grams
)
```

**2. Increase Training Data:**

If possible, augment corpus with medical literature:
- PubMed abstracts
- Clinical notes (de-identified)
- Medical textbooks

More occurrences → better embeddings.

**3. Use Pre-trained Medical Embeddings:**

Start with embeddings trained on large medical corpus:

```python
# Bio-WordVec, BioBERT, PubMedBERT
from transformers import AutoModel

model = AutoModel.from_pretrained("microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract")
```

Fine-tune on your specific medical domain.

**4. Increase Negative Sampling for Rare Words:**

Sample rare words more frequently as positive examples:

```python
# Custom sampling that favors rare words
def sample_training_pairs(corpus, word_freq):
    pairs = []
    for sentence in corpus:
        for i, center in enumerate(sentence):
            # More context samples for rare words
            window = 2 if word_freq[center] > 100 else 5
            for j in range(max(0, i-window), min(len(sentence), i+window+1)):
                if i != j:
                    pairs.append((center, sentence[j]))
    return pairs
```

**5. Use Subword Regularization (SentencePiece):**

Multiple segmentations for robustness:

```python
import sentencepiece as spm

# Train with subword regularization
spm.SentencePieceTrainer.train(
    input='medical_corpus.txt',
    model_prefix='medical_sp',
    vocab_size=32000,
    enable_differential_privacy=False,
    alpha=0.1  # Sampling parameter for subword regularization
)
```

**6. Hybrid Embeddings:**

Combine static and contextual:

```python
# For rare words, use FastText
# For common words, use fine-tuned BERT

def get_embedding(word, model_fasttext, model_bert, threshold=100):
    if word_frequency[word] < threshold:
        return model_fasttext[word]  # FastText handles rare words better
    else:
        # Use BERT for better contextualized embedding
        return get_bert_embedding(word, model_bert)
```

**Evaluation:**

Test on medical similarity tasks:
```python
# Medical word similarity dataset
test_pairs = [
    ("hypertension", "high blood pressure", 0.9),  # Similar
    ("diabetes", "hyperglycemia", 0.8),
    ("rhabdomyolysis", "cold", 0.1)  # Dissimilar
]

for w1, w2, expected_sim in test_pairs:
    sim = cosine_similarity(embed(w1), embed(w2))
    print(f"{w1} - {w2}: {sim:.2f} (expected: {expected_sim})")
```

---

#### Q3: You're deploying a model with 50K vocab × 1024-dim embeddings (50M parameters). Memory is constrained. What optimization strategies would you use?

**Answer:**

**Problem:**

Embeddings: 50,000 × 1024 × 4 bytes (FP32) = 200MB just for embeddings!

On edge device with 512MB total RAM → embeddings take 40%!

**Optimization Strategies:**

**1. Quantization:**

Reduce precision from FP32 (4 bytes) to INT8 (1 byte).

```python
import torch

# Original embeddings (FP32)
embeddings_fp32 = nn.Embedding(50000, 1024)  # 200MB

# Quantize to INT8
embeddings_int8 = torch.quantization.quantize_dynamic(
    embeddings_fp32, {nn.Embedding}, dtype=torch.qint8
)
# Now: 50MB (4× reduction!)
```

**Accuracy impact**: Minimal (< 1% performance drop for embeddings)

**2. Vocabulary Pruning:**

Remove rare words from vocabulary.

```python
# Keep only words appearing > N times
min_freq = 10
pruned_vocab = {word: idx for word, idx in vocab.items() 
                if word_freq[word] >= min_freq}

# 50K → 20K vocab (60% reduction)
# 200MB → 80MB
```

Handle OOV with subword fallback (byte-level encoding).

**3. Dimensionality Reduction:**

Reduce embedding dimension using PCA or random projection.

```python
from sklearn.decomposition import PCA

# Original: 1024-dim
embeddings_1024 = load_pretrained_embeddings()  # (50000, 1024)

# Reduce to 256-dim
pca = PCA(n_components=256)
embeddings_256 = pca.fit_transform(embeddings_1024)  # (50000, 256)

# 200MB → 50MB (4× reduction)
```

**Accuracy impact**: 5-10% performance drop (depends on task)

**4. Embedding Hashing (Hash Embeddings):**

Share parameters across words.

```python
class HashEmbedding(nn.Module):
    def __init__(self, vocab_size, embed_dim, num_buckets=10000):
        super().__init__()
        self.embeddings = nn.Embedding(num_buckets, embed_dim)
        self.num_buckets = num_buckets
    
    def forward(self, word_ids):
        # Hash word IDs to buckets
        hashed_ids = word_ids % self.num_buckets
        return self.embeddings(hashed_ids)

# Instead of 50K embeddings, use 10K
# 200MB → 40MB (5× reduction)
```

**Trade-off**: Collisions (different words share embeddings)

**5. Low-Rank Factorization:**

Decompose embedding matrix: E = U × V

```python
# E: (50000, 1024) = 200MB
# Decompose: E ≈ U (50000, 128) × V (128,  1024)

class FactorizedEmbedding(nn.Module):
    def __init__(self, vocab_size, embed_dim, rank=128):
        super().__init__()
        self.U = nn.Embedding(vocab_size, rank)
        self.V = nn.Linear(rank, embed_dim, bias=False)
    
    def forward(self, word_ids):
        u = self.U(word_ids)  # (batch, rank) 
        embed = self.V(u)  # (batch, embed_dim)
        return embed

# Memory: 50K×128 + 128×1024 = 6.4M + 131K = 6.5M params
# FP32: 26MB (8× reduction from 200MB!)
```

**6. Sparse Embeddings:**

Most embedding dimensions are near-zero.

```python
# Convert to sparse representation
embeddings_dense = model.embeddings.weight  # (50000, 1024)

# Threshold small values
embeddings_sparse = embeddings_dense.clone()
embeddings_sparse[torch.abs(embeddings_sparse) < threshold] = 0

# Store in sparse format (COO)
sparse_embeddings = embeddings_sparse.to_sparse()

# Memory savings depend on sparsity (typically 30-50% reduction)
```

**7. Mixed Precision:**

Use FP16 instead of FP32.

```python
embeddings_fp16 = embeddings_fp32.half()
# 200MB → 100MB (2× reduction)
```

**Minimal accuracy impact for embeddings.**

**Comparison Table:**

| Strategy | Memory Reduction | Accuracy Impact | Implementation |
|----------|------------------|-----------------|----------------|
| INT8 Quantization | 4× | < 1% | Easy |
| Vocab Pruning | 2-5× | Minimal (if rare words) | Easy |
| PCA (1024→256) | 4× | 5-10% | Medium |
| Hash Embeddings | 5-10× | 3-7% (collisions) | Medium |
| Low-Rank Factorization | 8× | 2-5% | Medium |
| FP16 | 2× | < 0.5% | Easy |

**Recommended Combination:**

1. FP16 (2×)
2. Low-rank factorization (4×)  
3. Total: 8× reduction (200MB → 25MB) with < 5% accuracy loss

---

#### Q4: Explain embedding anisotropy in BERT and why it's a problem.

**Answer:**

**What is Anisotropy?**

**Isotropic** embeddings: Uniformly distributed in embedding space (point in random directions).

**Anisotropic** embeddings: Clustered in small region of space (all point roughly same direction).

**BERT Embeddings are Anisotropic:**

```python
import torch
from transformers import BertModel, BertTokenizer

model = BertModel.from_pretrained('bert-base-uncased')
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

# Get embeddings for random words
words = ["king", "apple", "computer", "galaxy", "philosophy"]
embeddings = []

for word in words:
    inputs = tokenizer(word, return_tensors='pt')
    with torch.no_grad():
        outputs = model(**inputs)
        embed = outputs.last_hidden_state[0, 1, :]  # First real token
        embeddings.append(embed)

# Compute pairwise cosine similarities
from sklearn.metrics.pairwise import cosine_similarity

for i, w1 in enumerate(words):
    for j, w2 in enumerate(words):
        if i < j:
            sim = cosine_similarity(
                embeddings[i].unsqueeze(0),
                embeddings[j].unsqueeze(0)
            )
            print(f"{w1} - {w2}: {sim[0][0]:.3f}")

# Output (typical):
# king - apple: 0.78
# king - computer: 0.82
# apple - galaxy: 0.75
# ... all high similarity! (even for unrelated words)
```

**Why It's a Problem:**

1. **Poor Discrimination:**
   - Even unrelated words have high similarity (0.7-0.9)
   - Hard to distinguish truly similar from dissimilar

2. **Semantic Search Fails:**
   ```python
   # Query: "financial crisis"
   # Should return: "economic downturn", "recession"
   # Actually returns: almost everything (all have high similarity)
   ```

3. **Clustering Meaningless:**
   - K-means clustering on anisotropic embeddings groups randomly
   - All points roughly same distance from each other

**Why Does BERT Have Anisotropy?**

**Root Cause:** High-frequency tokens dominate training.

During BERT pre-training (MLM):
- Common words ("the", "a", "is") appear millions of times
- Model learns to predict them easily
- Their embeddings become very confident (high norm)
- Gradients push all embeddings toward this dominant direction

**Solutions:**

**1. Whitening (BERT-flow):**

Transform embeddings to have zero mean and identity covariance.

```python
def whiten_embeddings(embeddings):
    # embeddings: (num_samples, embed_dim)
    mean = embeddings.mean(dim=0, keepdim=True)
    centered = embeddings - mean
    
    # Compute covariance
    cov = torch.matmul(centered.T, centered) / len(embeddings)
    
    # Whitening transform
    U, S, V = torch.svd(cov)
    W = torch.matmul(U, torch.diag(1.0 / torch.sqrt(S)))
    
    whitened = torch.matmul(centered, W)
    return whitened
```

**2. Contrastive Learning (SimCSE):**

Train to maximize similarity of same sentence, minimize similarity of different sentences.

```python
# SimCSE training
sentence = "The cat sat on the mat"

# Two forward passes with dropout (different embeddings)
embed1 = model(sentence, dropout=0.1)
embed2 = model(sentence, dropout=0.1)

# Positive pair: (embed1, embed2) should be similar
# Negative pairs: (embed1, other sentences) should be dissimilar

loss = contrastive_loss(embed1, embed2, other_embeds)
```

**Result:** Embeddings spread more uniformly.

**3. Post-Processing (Average Subtraction):**

Simple but effective:

```python
def remove_projection(embeddings):
    # Subtract mean embedding
    mean_embed = embeddings.mean(dim=0, keepdim=True)
    return embeddings - mean_embed

# Or: subtract projection onto first principal component
def remove_pca(embeddings):
    pca = PCA(n_components=1)
    pc1 = pca.fit_transform(embeddings)
    projection = pc1 @ pca.components_
    return embeddings - projection
```

---

#### Q5: 

You need to create embeddings for a code search engine. How would your approach differ from natural language embeddings?

**Answer:**

**Challenges Unique to Code:**

1. **Syntax** vs semantics: `for (int i=0; i<n; i++)` vs `for i in range(n)` (same semantics, different syntax)
2. **Identifier names**: `calculateTotalPrice` vs `calc_total_price` vs `ctp`
3. **Code structure**: Indentation, brackets, operators
4. **Mixed modalities**: Code + comments + docstrings

**Approach:**

**1. Specialized Tokenization:**

```python
# Don't use standard NLP tokenizer
# Use code-aware tokenizer

from transformers import AutoTokenizer

# CodeBERT, GraphCodeBERT, CodeT5
tokenizer = AutoTokenizer.from_pretrained("microsoft/codebert-base")

# Preserves:
# - camelCase splitting: "calculateTotal" → ["calculate", "Total"]
# - Operators: "x+=1" → ["x", "+=", "1"]
# - Indentation (via special tokens or byte-level)
```

**2. Multi-Modal Embeddings:**

Combine code, docstrings, and comments:

```python
class CodeEmbedding(nn.Module):
    def __init__(self, code_encoder, text_encoder):
        super().__init__()
        self.code_encoder = code_encoder  # CodeBERT
        self.text_encoder = text_encoder  # BERT
        self.projection = nn.Linear(768*2, 768)
    
    def forward(self, code, docstring):
        code_eembed = self.code_encoder(code)
        text_embed = self.text_encoder(docstring)
        
        # Combine
        combined = torch.cat([code_embed, text_embed], dim=-1)
        return self.projection(combined)
```

**3. Contrastive Learning:**

Train to match code with its description:

```python
# Positive pair
code = "def add(a, b): return a + b"
description = "Adds two numbers and returns the result"

# Negative pairs
neg_code = "def multiply(a, b): return a * b"

# Maximize similarity of (code, description)
# Minimize similarity of (code, neg_description) and (neg_code, description)

loss = contrastive_loss(
    embed_code(code),
    embed_text(description),
    embed_code(neg_code),
    embed_text(neg_description)
)
```

**4. Structure-Aware:**

Use AST (Abstract Syntax Tree) or data flow:

```python
# GraphCodeBERT uses data flow graph
# Nodes: variables, functions
# Edges: data dependencies

# Example:
# def foo(x):
#     y = x + 1
#     return y * 2

# Graph:
# x → y (x flows to y)
# y → return (y flows to return value)
```

Graph neural network on top of Transformer!

**5. Evaluation:**

Code search specific metrics:

```python
# Test queries:
queries = [
    "function to read JSON file",
    "sort array in descending order",
    "connect to PostgreSQL database"
]

# Retrieve top-K code snippets for each query
# Measure:
# - MRR (Mean Reciprocal Rank): Where is first relevant result?
# - Recall@K: How many relevant results in top-K?

for query in queries:
    query_embed = embed_text(query)
    code_embeds = embed_all_code_snippets()
    
    # Cosine similarity
    similarities = cosine_similarity(query_embed, code_embeds)
    top_k_indices = similarities.arg sort()[::-1][:10]
    
    # Check if relevant code in top-K
    recall = evaluate_recall(top_k_indices, ground_truth[query])
```

**Best Practices:**

- Use pre-trained code model: CodeBERT, GraphCodeBERT, UniXcoder
- Fine-tune on your codebase (language-specific, domain-specific)
- Include both code and natural language in training
- Evaluate on real search queries from developers

---

### Production Challenges

**Challenge: Embedding Drift**

**Scenario:**

You trained embeddings on 2020 data. It's now 2024:
- New terms: "ChatGPT", "LLaMA", "RLHF" (not in vocab!)
- Meaning shift: "viral" (biology → social media)
- Model is stale

**Solutions:**

1. **Periodic Re-training:** Retrain embeddings quarterly on fresh data
2. **Incremental Updates:** Add new words, fine-tune existing
3. **OOV Handling:** Use FastText or byte-level encoding
4. **Monitoring:** Track OOV rate, alert when high

**Challenge: Embedding Bias**

Gender/race bias in embeddings:

```python
# Bias example
vector("doctor") - vector("nurse") ≈ vector("man") - vector("woman")
# Implies: doctor:male :: nurse:female (sexist!)
```

**Mitigation:**
- Balanced training data
- Debiasing algorithms (Hard Debiasing, INLP)
- Regular bias audits

---

### Key Takeaways for Interviews

1. **Understand Evolution:** Static → Contextual embeddings
2. **Trade-offs:** Memory, speed, accuracy
3. **Production Concerns:** Anisotropy, drift, bias, OOV
4. **Domain Adaptation:** Code, medical, multi-lingual require specialized approaches
5. **Optimization:** Quantization, pruning, factorization for deployment
