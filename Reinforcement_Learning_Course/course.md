# 40-Day Reinforcement Learning Course Syllabus

## Phase 1: RL Foundations (Days 1-10)
*   **Day 1:** Introduction to RL & MDPs (Agent, Env, State, Action, Reward, Markov Property).
*   **Day 2:** Bellman Equations (Expectation, Optimality, Value Functions).
*   **Day 3:** Dynamic Programming (Policy Evaluation, Policy Iteration, Value Iteration).
*   **Day 4:** Monte Carlo Methods (Prediction, Control, Exploration vs Exploitation).
*   **Day 5:** Temporal Difference Learning (TD(0), SARSA, Q-Learning).
*   **Day 6:** N-Step Bootstrap & Eligibility Traces (TD($\lambda$)).
*   **Day 7:** Function Approximation Basics (Linear methods, Feature construction).
*   **Day 8:** Multi-Armed Bandits (Epsilon-Greedy, UCB, Thompson Sampling).
*   **Day 9:** Exploration Strategies (Entropy, Intrinsic Motivation basics).
*   **Day 10:** RL Foundations Review & Mini-Project (GridWorld/Gym implementation from scratch).

## Phase 2: Value-Based Deep RL (Days 11-20)
*   **Day 11:** Deep Q-Networks (DQN) - Architecture, Replay Buffer, Target Network.
*   **Day 12:** Double DQN & Dueling DQN (Overestimation bias, Value/Advantage split).
*   **Day 13:** Prioritized Experience Replay (PER) (Importance Sampling).
*   **Day 14:** Noisy Nets & Distributional RL (C51, Quantile Regression).
*   **Day 15:** Rainbow DQN (Combining improvements).
*   **Day 16:** Recurrent RL (DRQN) for POMDPs.
*   **Day 17:** Deep Reinforcement Learning for Continuous Control (NAF - Normalized Advantage Functions).
*   **Day 18:** Practical DQN Tricks (Gradient Clipping, Reward Clipping, Frame Stacking).
*   **Day 19:** OpenAI Gym & Gymnasium Wrappers (Custom environments).
*   **Day 20:** Value-Based RL Review & Project (Atari Pong/Breakout).

## Phase 3: Policy-Based & Actor-Critic Methods (Days 21-30)
*   **Day 21:** Policy Gradients (REINFORCE, Log-Derivative Trick).
*   **Day 22:** Actor-Critic Fundamentals (Bias-Variance Tradeoff, Baseline).
*   **Day 23:** A2C & A3C (Asynchronous methods, Parallel environments).
*   **Day 24:** Generalized Advantage Estimation (GAE).
*   **Day 25:** Trust Region Methods (TRPO - Conjugate Gradient, KL constraint).
*   **Day 26:** Proximal Policy Optimization (PPO) - Clipped Objective.
*   **Day 27:** Deep Deterministic Policy Gradient (DDPG) - Continuous Actions.
*   **Day 28:** Twin Delayed DDPG (TD3) - Addressing function approximation error.
*   **Day 29:** Soft Actor-Critic (SAC) - Maximum Entropy RL.
*   **Day 30:** Policy-Based RL Review & Project (BipedalWalker/MuJoCo).

## Phase 4: Advanced Topics & Frontiers (Days 31-40)
*   **Day 31:** Model-Based RL (Dyna-Q, Planning, World Models).
*   **Day 32:** Monte Carlo Tree Search (MCTS) & AlphaGo/AlphaZero.
*   **Day 33:** Offline RL (Conservative Q-Learning - CQL, BCQ).
*   **Day 34:** Multi-Agent RL (MARL) - IQL, QMIX, MADDPG.
*   **Day 35:** Meta-RL (MAML, RL^2).
*   **Day 36:** Inverse Reinforcement Learning (IRL) & Imitation Learning (BC, GAIL).
*   **Day 37:** RLHF (Reinforcement Learning from Human Feedback) - PPO for LLMs.
*   **Day 38:** Decision Transformers (RL as Sequence Modeling).
*   **Day 39:** Distributed RL (Ray/RLLib, Apex).
*   **Day 40:** Final Course Review & Career Guide (Research vs Applied RL).
