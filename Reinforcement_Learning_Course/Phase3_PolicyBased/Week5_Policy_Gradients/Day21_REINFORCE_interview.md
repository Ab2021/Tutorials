# Day 21 Interview Questions: Policy Gradients

## Q1: What is the main difference between Value-Based and Policy-Based RL?
**Answer:**
*   **Value-Based (DQN):** Learns the value function $Q(s, a)$ and infers the policy (e.g., $\epsilon$-greedy or argmax). Indirect.
*   **Policy-Based (REINFORCE):** Learns the policy $\pi(a|s)$ directly by optimizing the parameters $\theta$ to maximize return. Direct.

## Q2: Why do Policy Gradients have high variance?
**Answer:**
The gradient estimate depends on the Monte Carlo return $G_t$.
$G_t$ is the sum of rewards over a long trajectory. Since the trajectory involves many stochastic transitions and actions, the value of $G_t$ can vary significantly between episodes, even for the same start state. This noise propagates into the gradient, causing the optimization to be unstable.

## Q3: How does a Baseline reduce variance?
**Answer:**
By subtracting a baseline $b(s)$ (usually the state value $V(s)$) from the return $G_t$, we center the learning signal.
$$ \nabla J \propto \nabla \log \pi (G_t - b(s)) $$
*   Without baseline: If rewards are always positive (e.g., +100, +110), we increase probability for ALL actions, just some more than others.
*   With baseline: We increase probability only for actions that perform *better than average* ($G_t > b$) and decrease for those *worse than average*. This zero-centered signal has much lower variance.

## Q4: Can REINFORCE be used for Off-Policy learning?
**Answer:**
No, standard REINFORCE is strictly **On-Policy**.
The expectation $\mathbb{E}_{\tau \sim \pi}$ assumes that the data was generated by the current policy $\pi_\theta$.
If we use old data (from a replay buffer), the distribution of trajectories matches the old policy, not the current one. The gradient estimate would be wrong.
(Note: Importance Sampling can be used to make it off-policy, but it introduces variance issues).

## Q5: What is the "Score Function" in Policy Gradients?
**Answer:**
The term $\nabla_\theta \log \pi_\theta(a|s)$.
It represents the direction in parameter space that increases the log-likelihood of selecting action $a$ in state $s$.
We multiply this direction by the reward $G_t$ to get the policy gradient.
