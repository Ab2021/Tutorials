# Day 4: Monte Carlo Methods

## 1. Learning from Experience
Unlike DP, Monte Carlo (MC) methods do not assume knowledge of the model ($P$ and $R$).
*   **Model-Free:** The agent learns directly from **episodes** of experience.
*   **Episodic:** MC works only for episodic tasks (tasks that terminate).
*   **Idea:** To estimate $V(s)$, simply average the returns observed after visiting state $s$.
    $$ V(s) \approx \frac{1}{N} \sum_{i=1}^N G_{i, s} $$

## 2. MC Prediction (Policy Evaluation)
Goal: Estimate $V_{\pi}(s)$ given episodes generated by $\pi$.
1.  Generate an episode: $S_0, A_0, R_1, S_1, A_1, R_2, ..., S_T$.
2.  Calculate return $G_t$ for each state visited.
3.  Update $V(S_t)$ towards $G_t$.
    $$ V(S_t) \leftarrow V(S_t) + \alpha (G_t - V(S_t)) $$
    Where $\alpha$ is the learning rate (or $1/N$ for simple average).

## 3. MC Control (Policy Improvement)
Goal: Find optimal $\pi_*$.
*   We need to estimate $Q(s, a)$ instead of $V(s)$ because we don't have the model to look ahead.
*   **Exploration:** To ensure we find the best action, we must explore.
*   **$\epsilon$-Greedy Policy:**
    *   With probability $1-\epsilon$: Choose greedy action $a = \arg\max Q(s, a)$.
    *   With probability $\epsilon$: Choose a random action.

## 4. Code Example: MC Prediction for Blackjack
We'll simulate a simplified Blackjack game to estimate the value of holding a sum of 20.

```python
import numpy as np
import random

# Simplified Blackjack: 
# State: (PlayerSum, DealerShowing, UsableAce)
# Action: 0 (Stick), 1 (Hit)

def play_episode(policy):
    # Simulate one game (simplified logic)
    player_sum = random.randint(12, 21)
    dealer_card = random.randint(1, 10)
    trajectory = []
    
    while True:
        state = player_sum
        action = policy(state)
        trajectory.append((state, action))
        
        if action == 0: # Stick
            # Dealer plays (simplified: dealer sticks on 17+)
            dealer_sum = dealer_card
            while dealer_sum < 17: dealer_sum += random.randint(1, 10)
            
            if dealer_sum > 21 or player_sum > dealer_sum: reward = 1
            elif player_sum == dealer_sum: reward = 0
            else: reward = -1
            break
        else: # Hit
            player_sum += random.randint(1, 10)
            if player_sum > 21:
                reward = -1
                break
                
    return trajectory, reward

# MC Prediction
V = {s: 0.0 for s in range(12, 22)}
Returns = {s: [] for s in range(12, 22)}
policy = lambda s: 0 if s >= 20 else 1 # Stick on 20+, Hit otherwise

for _ in range(10000):
    traj, reward = play_episode(policy)
    for state, action in traj:
        if state in V:
            Returns[state].append(reward)
            V[state] = np.mean(Returns[state])

print("Value of holding 20:", V[20])
print("Value of holding 19:", V[19])
```

### Key Takeaways
*   MC learns from complete episodes.
*   No bootstrapping (estimates are independent).
*   Requires exploration ($\epsilon$-greedy) for control.
