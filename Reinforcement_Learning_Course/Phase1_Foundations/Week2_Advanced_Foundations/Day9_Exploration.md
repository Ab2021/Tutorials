# Day 9: Exploration Strategies

## 1. Beyond $\epsilon$-Greedy
$\epsilon$-Greedy is "dithering" (random noise). It doesn't know *where* to explore, just *that* it should explore.
In sparse reward environments (e.g., Montezuma's Revenge), random walking rarely finds the goal. We need **Directed Exploration**.

## 2. Entropy Regularization
In Policy Gradient methods (Day 21), we often add an entropy term to the loss function.
$$ L = L_{policy} + \beta H(\pi(\cdot|s)) $$
$$ H(\pi) = - \sum_a \pi(a|s) \log \pi(a|s) $$
*   **Goal:** Maximize entropy (randomness) of the policy.
*   **Effect:** Prevents the policy from becoming deterministic too early (premature convergence).

## 3. Intrinsic Motivation
We can augment the reward signal:
$$ R_{total} = R_{extrinsic} + \lambda R_{intrinsic} $$
*   **Extrinsic:** From the environment (score, win/loss).
*   **Intrinsic:** Generated by the agent (curiosity, novelty).
    *   "I haven't seen this state before -> High Reward".

## 4. Code Example: Entropy Calculation
```python
import numpy as np
import matplotlib.pyplot as plt

def calculate_entropy(probs):
    # Avoid log(0)
    probs = np.clip(probs, 1e-10, 1.0)
    return -np.sum(probs * np.log(probs))

# Deterministic Policy (Low Entropy)
p1 = np.array([0.9, 0.05, 0.05])
print(f"Entropy of {p1}: {calculate_entropy(p1):.3f}")

# Uniform Policy (High Entropy)
p2 = np.array([0.33, 0.33, 0.34])
print(f"Entropy of {p2}: {calculate_entropy(p2):.3f}")

# Visualization of Entropy for 2-action policy (p, 1-p)
probs = np.linspace(0, 1, 100)
entropies = [-p*np.log(p+1e-10) - (1-p)*np.log(1-p+1e-10) for p in probs]

# plt.plot(probs, entropies)
# plt.title("Entropy of Bernoulli Distribution")
```

### Key Takeaways
*   Random exploration is inefficient for hard problems.
*   Entropy encourages "keeping options open".
*   Intrinsic motivation rewards "curiosity".
