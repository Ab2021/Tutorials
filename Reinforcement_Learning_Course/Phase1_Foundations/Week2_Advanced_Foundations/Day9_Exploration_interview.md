# Day 9 Interview Questions: Exploration Strategies

## Q1: Why is $\epsilon$-greedy exploration often insufficient for complex environments?
**Answer:**
$\epsilon$-greedy relies on random dithering. In environments with **sparse rewards** (e.g., a maze where the goal is far away), the probability of randomly stumbling upon the goal decays exponentially with the distance. Directed exploration is needed to actively seek out novel states.

## Q2: Explain the "TV Problem" in Curiosity-Driven Learning.
**Answer:**
Curiosity is often defined as the error in predicting the next state.
If the environment contains stochastic or chaotic elements (like a TV showing static noise), the prediction error will always be high, regardless of how much the agent observes it.
The agent receives high intrinsic reward and gets "addicted" to staring at the noise, stopping exploration of the rest of the world.

## Q3: What is the advantage of Noisy Nets (Parameter Noise) over Action Noise?
**Answer:**
Action noise (like $\epsilon$-greedy or adding Gaussian noise to actions) is uncorrelated across time steps. The agent might jitter back and forth, canceling out its own exploration.
Parameter noise (Noisy Nets) perturbs the policy itself. This results in **state-dependent, temporally consistent** exploration. The agent might commit to a specific strategy (e.g., "always go left") for an episode, allowing it to explore deep into the environment.

## Q4: How does Entropy Regularization encourage exploration?
**Answer:**
By adding an entropy term $H(\pi)$ to the objective function, we penalize policies that are too confident (deterministic).
This forces the optimizer to maintain a probability mass on all actions unless one action is clearly superior. It prevents premature convergence to a sub-optimal deterministic policy.

## Q5: What is the difference between Extrinsic and Intrinsic motivation?
**Answer:**
*   **Extrinsic:** Defined by the environment/designer (e.g., game score, winning/losing).
*   **Intrinsic:** Generated by the agent's internal drive (e.g., curiosity, novelty, empowerment). It helps the agent learn when extrinsic rewards are sparse or absent.
