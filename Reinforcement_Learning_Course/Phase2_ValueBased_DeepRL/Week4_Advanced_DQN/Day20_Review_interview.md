# Day 20 Interview Questions: Phase 2 Review

## Q1: Compare DQN, Double DQN, and Dueling DQN.
**Answer:**
*   **DQN:** Base algorithm. Uses Target Network and Replay Buffer. Suffers from overestimation.
*   **Double DQN:** Fixes overestimation by decoupling selection and evaluation ($Q(s, \arg\max Q_{online}, \theta^-)$).
*   **Dueling DQN:** Changes architecture to $V(s) + A(s, a)$. Improves sample efficiency by learning state values independently of actions.

## Q2: What is the "Deadly Triad" in Deep RL?
**Answer:**
Instability occurs when we combine three elements:
1.  **Function Approximation:** Using Neural Networks (generalization).
2.  **Bootstrapping:** Updating estimates based on other estimates (TD learning).
3.  **Off-Policy Learning:** Training on data generated by a different policy (Replay Buffer).
DQN has all three, which is why tricks like Target Networks and Huber Loss are needed to stabilize it.

## Q3: Why is Prioritized Experience Replay (PER) useful?
**Answer:**
It improves data efficiency by replaying "surprising" transitions (high TD error) more frequently.
To correct the bias introduced by non-uniform sampling, we must use Importance Sampling weights $w_i = (N \cdot P(i))^{-\beta}$.

## Q4: How does Distributional RL (C51) differ from standard RL?
**Answer:**
Standard RL learns the *mean* of the return. Distributional RL learns the *full probability distribution* of the return.
This provides a richer training signal (auxiliary task) and helps with stability and risk-sensitive decision making.

## Q5: What is the main limitation of Value-Based methods that Policy Gradients solve?
**Answer:**
Value-Based methods struggle with **Continuous Action Spaces** (finding $\max_a Q$ is hard) and cannot learn **Stochastic Policies** (they are deterministic). Policy Gradients handle both naturally.
