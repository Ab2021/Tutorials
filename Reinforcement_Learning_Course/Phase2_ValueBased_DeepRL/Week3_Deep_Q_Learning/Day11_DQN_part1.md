# Day 11 Deep Dive: Why DQN Works (and when it doesn't)

## 1. The Stability Problem
Before DQN, applying neural networks to Q-Learning was considered unstable.
*   **Correlated Data:** In RL, $S_{t+1}$ is highly correlated with $S_t$. Neural networks assume i.i.d. data. Training on a sequence of correlated samples leads to overfitting to the current local trajectory and forgetting the rest of the state space (**Catastrophic Forgetting**).
*   **Moving Targets:** The target $y = r + \gamma \max Q(s', a'; \theta)$ changes every time we update $\theta$. This is like trying to hit a bullseye that moves every time you throw a dart.

## 2. Experience Replay Details
*   **Capacity:** The buffer size is a hyperparameter (e.g., $10^6$).
    *   **Too Small:** Data becomes correlated again; we only remember the recent past.
    *   **Too Large:** We train on very old data generated by a much worse policy (Policy Lag). This is usually fine for off-policy DQN but can be an issue.
*   **Sampling:** Uniform random sampling is the baseline. (See Day 13 for Prioritized Replay).

## 3. Target Network Update Frequency
How often should we update $\theta^- \leftarrow \theta$?
*   **Hard Update:** Copy weights every $C$ steps (e.g., 1000).
    *   Pros: Very stable target for $C$ steps.
    *   Cons: Learning pauses/lags for $C$ steps.
*   **Soft Update (Polyak Averaging):** Update every step by a small amount $\tau$.
    $$ \theta^- \leftarrow \tau \theta + (1-\tau) \theta^- $$
    *   Standard in continuous control (DDPG), but Hard Update is often preferred for DQN on Atari.

## 4. The Huber Loss
Instead of MSE, DQN often uses **Huber Loss** (Smooth L1 Loss).
$$ L(\delta) = \begin{cases} \frac{1}{2}\delta^2 & \text{for } |\delta| \le 1 \\ |\delta| - \frac{1}{2} & \text{otherwise} \end{cases} $$
*   **Reason:** Gradients of MSE grow linearly with error. If the Q-value estimate is wildly wrong (common early in training), gradients explode. Huber loss clips the gradient to $\pm 1$, preventing instability.
