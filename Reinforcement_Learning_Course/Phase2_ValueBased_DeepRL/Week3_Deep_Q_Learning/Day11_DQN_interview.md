# Day 11 Interview Questions: Deep Q-Networks (DQN)

## Q1: What are the two main contributions of the original DQN paper (Mnih et al., 2015)?
**Answer:**
1.  **Experience Replay:** Storing transitions $(s, a, r, s')$ in a buffer and sampling random batches for training. This breaks temporal correlations and improves data efficiency.
2.  **Target Network:** Using a separate, slowly updating network to calculate the TD target. This stabilizes the learning objective.

## Q2: Why do we need a Target Network?
**Answer:**
In Q-Learning, the target $y$ depends on the same parameters $\theta$ we are optimizing.
$$ y = r + \gamma \max_{a'} Q(s', a'; \theta) $$
Without a target network, increasing $Q(s, a)$ often inadvertently increases $Q(s', a')$ as well (due to generalization). This causes the target to move in the same direction as the prediction, leading to positive feedback loops and divergence. The target network fixes the target for a period, turning the problem into a supervised regression task.

## Q3: Can DQN handle continuous action spaces?
**Answer:**
No, not directly.
DQN requires computing $\max_a Q(s, a)$.
*   **Discrete Actions:** We can simply output $k$ values and take the max.
*   **Continuous Actions:** Finding the max of a complex non-linear function $Q(s, a)$ with respect to $a$ is an optimization problem itself (slow).
For continuous actions, we use Actor-Critic methods (DDPG, SAC) or Normalized Advantage Functions (NAF).

## Q4: What is "Catastrophic Forgetting" in the context of RL?
**Answer:**
It occurs when a neural network learns new information but completely forgets previously learned information.
In RL, if an agent moves to a new room and trains only on samples from that room, it might forget how to navigate the previous room. Experience Replay mitigates this by keeping old samples in the buffer and mixing them with new ones.

## Q5: Why is the Replay Buffer usually a FIFO queue (First-In-First-Out)?
**Answer:**
We want to train on a diverse set of experiences, but we also want the data to be somewhat relevant to the current policy.
A FIFO queue of fixed size (e.g., 1M frames) naturally discards very old data (generated by a very different, likely random policy) while keeping a large window of recent history.
