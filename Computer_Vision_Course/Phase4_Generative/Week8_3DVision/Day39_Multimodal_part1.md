# Day 39 Deep Dive: LLaVA & BLIP

## 1. BLIP (Bootstrapping Language-Image Pre-training)
**Problem:** Web data is noisy (bad captions).
**Solution:**
*   **Captioner:** A model trained to generate captions for images.
*   **Filter:** A model trained to check if (Image, Text) match.
*   **Bootstrap:** Use the Captioner to generate synthetic captions for noisy web images, then filter the best ones.
*   **BLIP-2:** Uses a "Q-Former" (Query Transformer) to bridge a frozen Image Encoder and a frozen LLM.

## 2. LLaVA (Large Language and Vision Assistant)
**Idea:** Visual Instruction Tuning.
*   **Architecture:** CLIP ViT-L/14 + Linear Projection + Vicuna (LLM).
*   **Data:** Generated by GPT-4.
    *   Input: Image + Bounding Boxes + Captions.
    *   GPT-4 Output: Complex conversation about the image.
*   **Training:** Fine-tune the LLM to answer questions about the image features.
*   **Result:** An open-source "GPT-4 Vision" competitor.

## 3. SAM (Segment Anything Model)
**A Foundation Model for Segmentation.**
*   **Task:** Promptable Segmentation.
*   **Input:** Image + Prompt (Point, Box, Text).
*   **Output:** Mask.
*   **Architecture:**
    *   **Image Encoder:** MAE (Masked Autoencoder) ViT-H. Heavy. Runs once.
    *   **Prompt Encoder:** Lightweight.
    *   **Mask Decoder:** Lightweight. Runs in milliseconds.
*   **Data:** SA-1B dataset (1 Billion masks).

## 4. Open-Vocabulary Detection (GLIP)
**Grounded Language-Image Pre-training.**
*   Treat object detection as a phrase grounding task.
*   "Find the person and the kite."
*   Aligns region features (boxes) with word features.
*   Can detect objects it has never seen before if you provide the name.

## Summary
The trend is towards **General Purpose Foundation Models**. Instead of training a specific model for "Car Detection", we use a VLM that can detect/segment anything based on language prompts.
