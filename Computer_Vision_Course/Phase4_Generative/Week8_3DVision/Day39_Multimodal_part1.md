# Day 39 Deep Dive: LLaVA & BLIP

## 1. BLIP (Bootstrapping Language-Image Pre-training)
**Problem:** Web data is noisy (bad captions).
**Solution:**
*   **Captioner:** A model trained to generate captions for images.
*   **Filter:** A model trained to check if (Image, Text) match.
*   **Bootstrap:** Use the Captioner to generate synthetic captions for noisy web images, then filter the best ones.
*   **BLIP-2:** Uses a "Q-Former" (Query Transformer) to bridge a frozen Image Encoder and a frozen LLM.

## 2. LLaVA (Large Language and Vision Assistant)
**Idea:** Visual Instruction Tuning.
*   **Architecture:** CLIP ViT-L/14 + Linear Projection + Vicuna (LLM).
*   **Data:** Generated by GPT-4.
    *   Input: Image + Bounding Boxes + Captions.
    *   GPT-4 Output: Complex conversation about the image.
*   **Training:** Fine-tune the LLM to answer questions about the image features.
*   **Result:** An open-source "GPT-4 Vision" competitor.

## 3. SAM (Segment Anything Model)
**A Foundation Model for Segmentation.**
*   **Task:** Promptable Segmentation.
*   **Input:** Image + Prompt (Point, Box, Text).
*   **Output:** Mask.
# Day 39 Deep Dive: LLaVA & BLIP

## 1. BLIP (Bootstrapping Language-Image Pre-training)
**Problem:** Web data is noisy (bad captions).
**Solution:**
*   **Captioner:** A model trained to generate captions for images.
*   **Filter:** A model trained to check if (Image, Text) match.
*   **Bootstrap:** Use the Captioner to generate synthetic captions for noisy web images, then filter the best ones.
*   **BLIP-2:** Uses a "Q-Former" (Query Transformer) to bridge a frozen Image Encoder and a frozen LLM.

## 2. LLaVA (Large Language and Vision Assistant)
**Idea:** Visual Instruction Tuning.
*   **Architecture:** CLIP ViT-L/14 + Linear Projection + Vicuna (LLM).
*   **Data:** Generated by GPT-4.
    *   Input: Image + Bounding Boxes + Captions.
    *   GPT-4 Output: Complex conversation about the image.
*   **Training:** Fine-tune the LLM to answer questions about the image features.
*   **Result:** An open-source "GPT-4 Vision" competitor.

## 3. SAM (Segment Anything Model)
**A Foundation Model for Segmentation.**
*   **Task:** Promptable Segmentation.
*   **Input:** Image + Prompt (Point, Box, Text).
*   **Output:** Mask.
*   **Architecture:**
    *   **Image Encoder:** MAE (Masked Autoencoder) ViT-H. Heavy. Runs once.
    *   **Prompt Encoder:** Lightweight.
    *   **Mask Decoder:** Lightweight. Runs in milliseconds.
*   **Data:** SA-1B dataset (1 Billion masks).

## 4. Grounding DINO (Open-Set Detection)
**The SOTA for Zero-Shot Detection.**
*   **Concept:** Fuses a Transformer-based detector (DINO) with language features.
*   **Mechanism:**
    *   **Feature Fusion:** Visual features and Text features are fused at multiple layers using Cross-Attention.
    *   **Query Selection:** The model selects "queries" based on the text prompt.
*   **Application:** Can detect "a person wearing a red hat" without ever seeing that specific class during training.
*   **Pipeline:** Image + Text Prompt $\to$ Bounding Boxes + Phrases.

## 5. Multimodal Agents
**Beyond Static Tasks.**
*   **Concept:** Give an LLM access to Vision Tools (ReAct Pattern).
*   **Tools:**
    *   `describe_image(img)` $\to$ Caption (BLIP).
    *   `detect_objects(img)` $\to$ Boxes (Grounding DINO).
    *   `segment_object(img, box)` $\to$ Mask (SAM).
*   **Workflow:**
    1.  **User:** "How many cars are in this image?"
    2.  **Agent:** "I need to count cars. I will use `detect_objects` with prompt 'car'."
    3.  **Tool Output:** "[Box1, Box2, Box3]"
    4.  **Agent:** "There are 3 cars."

## 6. Video-LLMs (Video-LLaVA)
**Extending to Time.**
*   **Challenge:** Video is heavy (many frames).
*   **Solution:**
    *   **Sampling:** Sample 8-16 frames uniformly.
    *   **Encoding:** Encode each frame with CLIP/ViT.
    *   **Pooling:** Average or concatenate frame features $\to$ Video Token.
    *   **Input:** Video Tokens + Text Tokens $\to$ LLM.
*   **Capability:** "What happened in this video?" "Why is the person laughing?"

## 7. Open-Vocabulary Detection (GLIP)
**Grounded Language-Image Pre-training.**
*   Treat object detection as a phrase grounding task.
*   "Find the person and the kite."
*   Aligns region features (boxes) with word features.
*   Can detect objects it has never seen before if you provide the name.

## Summary
The field is moving from **Specialized Models** (just detection) to **General Purpose Multimodal Agents** that can perceive, reason, and act.
