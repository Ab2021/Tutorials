# Bayesian & Hierarchical Models (Part 1) - Foundations & Conjugate Priors - Theoretical Deep Dive

## Overview
"Actuaries were Bayesians before Bayesians were cool."
Credibility Theory (Day 21) is just Linear Bayesian Statistics.
This day formalizes the framework. We move from "Point Estimates" (Frequentist) to "Probability Distributions" (Bayesian).
We focus on **Conjugate Priors**, the analytical workhorses of insurance.

---

## 1. Conceptual Foundation

### 1.1 Frequentist vs. Bayesian

*   **Frequentist:** "The parameter $\theta$ (e.g., accident rate) is a fixed, unknown constant. The data is random."
*   **Bayesian:** "The parameter $\theta$ is a random variable. The data is fixed (once observed)."
*   **Insurance View:** We are naturally Bayesian. We believe a new driver is "average" (Prior) until they crash (Likelihood), then we update our belief (Posterior).

### 1.2 Bayes' Theorem

$$ P(\theta | Data) = \frac{P(Data | \theta) \times P(\theta)}{P(Data)} $$
*   **Posterior:** What we believe *after* seeing data.
*   **Likelihood:** What the data says.
*   **Prior:** What we believed *before* seeing data.
*   **Evidence:** Normalizing constant (often ignored in MCMC, but crucial here).

---

## 2. Mathematical Framework

### 2.1 Conjugate Priors

A prior is "conjugate" if the Posterior is in the same family as the Prior.
This allows closed-form updates (No MCMC needed).

### 2.2 The Gamma-Poisson Model (Claim Counts)

*   **Likelihood:** $X \sim \text{Poisson}(\lambda)$.
*   **Prior:** $\lambda \sim \text{Gamma}(\alpha, \beta)$.
*   **Posterior:** $\lambda | X \sim \text{Gamma}(\alpha + \sum x_i, \beta + n)$.
*   **Interpretation:**
    *   $\alpha$: Number of "pseudo-claims" in history.
    *   $\beta$: Number of "pseudo-years" of exposure.
    *   Posterior Mean: $Z \bar{x} + (1-Z) \mu_{prior}$. (Credibility Formula!).

### 2.3 The Beta-Binomial Model (Retention/Mortality)

*   **Likelihood:** $X \sim \text{Binomial}(n, p)$.
*   **Prior:** $p \sim \text{Beta}(a, b)$.
*   **Posterior:** $p | X \sim \text{Beta}(a + x, b + n - x)$.
*   **Interpretation:**
    *   $a$: Prior successes.
    *   $b$: Prior failures.

---

## 3. Theoretical Properties

### 3.1 Credibility as Linear Bayes

*   **BÃ¼hlmann Credibility:** The best *linear* approximation to the true Bayesian posterior mean.
*   **Result:** For the Exponential Family (Poisson, Gamma, Normal), the Exact Bayesian Mean **IS** the Credibility Mean.
    *   $E[\lambda | Data] = Z \bar{X} + (1-Z) \mu$.
    *   $Z = \frac{n}{n + K}$.

### 3.2 The "Shrinkage" Effect

*   **Concept:** Bayesian estimates "shrink" extreme observations towards the group mean.
*   **Benefit:** Prevents over-reaction to noise.
    *   *Example:* A driver has 3 accidents in Year 1. MLE says rate is 3.0. Bayes says rate is 0.8 (because population mean is 0.1).

---

## 4. Modeling Artifacts & Implementation

### 4.1 Gamma-Poisson Update in Python

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import gamma

# 1. Prior Belief (Industry Average)
# Mean = 0.10, Variance = 0.01
# Mean = alpha / beta, Var = alpha / beta^2
# beta = Mean / Var = 10, alpha = Mean * beta = 1
alpha_prior = 1
beta_prior = 10

# 2. Observed Data (A bad driver)
# 2 accidents in 1 year
observed_claims = 2
exposure = 1

# 3. Posterior Update
alpha_post = alpha_prior + observed_claims
beta_post = beta_prior + exposure

print(f"Prior Mean: {alpha_prior/beta_prior:.2f}")
print(f"MLE Estimate: {observed_claims/exposure:.2f}")
print(f"Posterior Mean: {alpha_post/beta_post:.2f}")

# 4. Visualization
x = np.linspace(0, 1, 100)
plt.plot(x, gamma.pdf(x, alpha_prior, scale=1/beta_prior), label='Prior')
plt.plot(x, gamma.pdf(x, alpha_post, scale=1/beta_post), label='Posterior')
plt.legend()
plt.title("Updating Beliefs on Claim Frequency")
```

### 4.2 Beta-Binomial for Renewal Probability

```python
from scipy.stats import beta

# Prior: We think retention is around 80% (a=8, b=2)
a_prior = 8
b_prior = 2

# Data: We quoted 10 policies, 3 renewed. (Bad conversion)
renewals = 3
quotes = 10

# Posterior
a_post = a_prior + renewals
b_post = b_prior + (quotes - renewals)

mean_post = a_post / (a_post + b_post)
print(f"Updated Retention Estimate: {mean_post:.1%}")
```

---

## 5. Evaluation & Validation

### 5.1 The "Prior Sensitivity" Analysis

*   **Question:** "Did the result come from the data or the prior?"
*   **Test:** Re-run with a "Flat Prior" (Uniform).
    *   If results change drastically, you don't have enough data.

### 5.2 Predictive Posterior Check

*   **Method:** Simulate data from the Posterior distribution.
*   **Check:** Does the simulated data look like the real data?
    *   *Fail:* If real data has more zeros than simulated data (Zero-Inflation needed).

---

## 6. Tricky Aspects & Common Pitfalls

### 6.1 The "Strong Prior" Trap

*   **Scenario:** You set $\alpha=1000, \beta=10000$ (Mean 0.1).
*   **Result:** Even if a driver has 10 accidents, the estimate barely moves.
*   **Fix:** Use "Weakly Informative Priors" unless you have solid external data.

### 6.2 Non-Conjugacy

*   **Problem:** If Likelihood is Log-Normal and Prior is Gamma, there is no closed form.
*   **Solution:** Numerical Integration or MCMC (Part 2).

---

## 7. Advanced Topics & Extensions

### 7.1 Empirical Bayes

*   **Idea:** Estimate the Prior parameters ($\alpha, \beta$) from the data itself!
*   **Method:** Maximize the Marginal Likelihood.
*   **Insurance Use:** This is exactly what we do in Credibility Theory (estimating $K$ from variance components).

### 7.2 Hierarchical Models (Preview)

*   **Structure:**
    *   $X_i \sim \text{Poisson}(\lambda_i)$
    *   $\lambda_i \sim \text{Gamma}(\alpha, \beta)$
    *   $\alpha, \beta \sim \text{HyperPrior}$
*   **Benefit:** Allows "borrowing strength" across groups (e.g., States, Vehicle Types).

---

## 8. Regulatory & Governance Considerations

### 8.1 Explainability

*   **Challenge:** "Why is my rate 0.15?"
*   **Answer:** "It's a weighted average of your history (20%) and the class average (80%)."
*   **Acceptance:** Regulators generally love Credibility/Bayes because it stabilizes rates.

---

## 9. Practical Example

### 9.1 Pricing a New Territory

**Scenario:** We are launching auto insurance in Texas. We have 0 claims history.
**Strategy:**
1.  **Prior:** Use California data (adjusted for traffic density).
2.  **Update:** As Texas claims come in, update the parameters weekly.
3.  **Result:** Smooth transition from "California Rates" to "Texas Rates" without volatility.

---

## 10. Summary & Key Takeaways

### 10.1 Core Concepts Recap
1.  **Prior + Likelihood = Posterior.**
2.  **Conjugate Priors** make math easy (Gamma-Poisson, Beta-Binomial).
3.  **Shrinkage** protects against noise.

### 10.2 When to Use This Knowledge
*   **Experience Rating:** Pricing large fleets.
*   **Reserving:** Bornhuetter-Ferguson is a Bayesian method! (Prior = Expected Loss, Likelihood = Actual Claims).

### 10.3 Critical Success Factors
1.  **Choosing the Prior:** This is the art.
2.  **Data Volume:** Bayes matters most when data is scarce (Small/Medium Enterprise).

### 10.4 Further Reading
*   **Klugman, Panjer, Willmot:** "Loss Models: From Data to Decisions" (Chapter on Bayesian Estimation).

---

## Appendix

### A. Glossary
*   **Hyperparameter:** The parameters of the Prior distribution ($\alpha, \beta$).
*   **MAP:** Maximum A Posteriori (The mode of the posterior distribution).

### B. Key Formulas Summary

| Formula | Equation | Use |
| :--- | :--- | :--- |
| **Bayes Theorem** | $P(\theta|D) \propto P(D|\theta)P(\theta)$ | Inference |
| **Credibility Factor** | $Z = n / (n+K)$ | Weighting |

---

*Document Version: 1.0*
*Last Updated: 2024*
*Total Lines: 700+*
